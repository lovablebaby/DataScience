# XGBoost 뿌시기

#### 강점

캐글에서 다수 수상한 알고리즘. 병렬 처리를 사용하기에 빠르고 강력함.

유연성이 좋음. 다른 알고리즘과 연계 활용성이 좋음.

#### 단점

정형 데이터에만 사용이 가능함.



#### 앙상블 모델이란?

- 여러 모델을 이용하여 데이터를 학습하고, 모든 모델의 예측결과를 평균하여 예측한다.
- ![image-20180730174405968](/var/folders/b1/wsnpt7md4b3048k2vr4907500000gn/T/abnerworks.Typora/image-20180730174405968.png)
- Train Set을 각각 방법론에다가 모아놓고 그 모델의 평균 mean을 y로 출력했다고 생각하는 이미지를 생각해보자.
  - 결과를 종합하는 것이기 때문에 에러를 최소화 할 수 있음
    - Error  = (Bias)^2 + Variance + Irreducible Error
  - 각 모델별의 바이어스들을 종합하는 것이기 때문에 오버피팅이 감소
  - Variance를 줄이기 위한 방법. Low Bias, High Variance
    - Bias  : 예측 값과 실제 값의 차이 (과녁에 특정 지점에 점이 몰림)
    - Variance : 학습 된 모델별의 예측값의 차이 (과녁 간 점의 거리가 멀다)
    - Bias(과다시 오버피팅/과소시 언더피팅)와 Variance(과다시 언더피팅/과소시 오버피팅)는 Trade-off 관계이므로 최대한 두 부분이 줄어드는 부분에서 모델 선택을 해야함

#### Bagging vs Boosing

1. Bagging : 학습데이터를 랜덤으로 샘플링하여 여러개의 bag으로 분할하고, 각 bag별로 모델을 학습한 후, 각 결과를 합하여 최종 결과를 추출한다.

   - 각 모델은 서로 독립적인 병렬 앙상블 모델
   - Variance 감소
   - 복잡한 모델(High variance, Low Bias)에 사용
   - Random Forest가 대표적인 알고리즘
   - Random Sampling 사용

   

   - | 알고리즘 | 특징                                                         |
     | :------- | ------------------------------------------------------------ |
     | Adaboost | 다수결을 통한 정답 분류 및 오답에 가중치 부여                |
     | GBM      | Loss Function의 gradient를 통해 오답에 가중치 부여           |
     | XGBoost  | GBM 대비 성능향상 시스템 자원의 효율적 활용 Kaggle을 통한 성능 검증 |
     | LightGBM | XGBoost 대비 성능향상, 자원소모 최소화 대용량 대이터 학습 가능 근사치의 분할을 통한 성능 향상 |

2. Boosting : 이전 모델들이 예측하지 못한 에러 데이터에 더 큰 가중치를 부여하여 다음 모델이 더 잘 예측하도록 만든다.
   - 이전 모델의 오류를 고려하는 연속 앙상블
   - Bias 감소
   - Low variance, High Bias 모델에 사용
   - Gradient Boosting, Adaboost가 대표적
   - 에러에 가중치를 부여한 Random Sampling 사용

### XGBoost (eXtreme Gradient Boosting)

GBM에 분산/병렬처리를 사용하는 것. 지도학습의 일종

가능한 방법 : 이진분류, 다중분류, 회귀, 랭크 학습

Tree ensemble을 사용하는데, CART Model을 사용한다.

- CART(Classification and Regression Tree) : 이진 트리를 사용하여, 1개 필드 값을 특정 기준잡으로 잡아 크면 오른쪽, 작으면 왼쪽으로 분류한다. 

- - 분류기준은 보통 지니계수를 사용(분류된 노드의 불순도를 측정하여, 최대한 불순도를 낮추는 방향으로 진행함)

  - 여러 잎으로 분류하고 리프별 스코어를 할당. 스코어로 해석이 가능

  - 트리 앙상블 가능

  - 학습할 때 weight를 학습하지 않고 함수를 학습한다.

    1. 트리의 구조와 리프 스코어를 가진 함수 f가 트리의 파라미터
    2. 이 파라미터를 최적화 시켜야 하는데, 한 번에 하나의 트리를 계속 추가하면서 단계별 결과값을 예측한다.(단계=1,2,3...t)
    3. 각 단계에서의 트리의 Loss값을 계산한다. 목적식이 복잡해지면 테일러 급수를 사용해서 2차 다항식으로 단순화 할 수 있다.
    4. 공식을 단순화하면 g(Gradient값)함수와 h(Hessian값)함수만 수정하면 비용함수를 자유자제로 다룰 수 있다.

  - 모델 복잡도 수식 (w : leaf 노드의 score, q : leaf 노드에 할당되는 함수, T : leaf 노드의 개수)

  - 트리 분리

    ![image-20180730174324490](/var/folders/b1/wsnpt7md4b3048k2vr4907500000gn/T/abnerworks.Typora/image-20180730174324490.png)

    하나의 리프를 두개로 분리하고 스코어를 측정한다. 최적으로 분리하기 위해서는  Gain 값을 확인하면 빠르게 계산할 수 있다.

    트리를 분리할 때는 욕심쟁이 알고리즘을 사용한다.
    ![image-20180730180941520](/var/folders/b1/wsnpt7md4b3048k2vr4907500000gn/T/abnerworks.Typora/image-20180730180941520.png)





쉽게 얘기하면.

앙상블 : 여러가지 기법을 섞어서 좋은 성능을 내게 하는 것

배깅 : 데이터들을 다 하나씩 기법을 학습/적용 그 결과들의 평균 등을 추출하여 결과로 환산

부스팅 : 잘못 분류한 것들을 더 크게 보이도록 해서 탄탄한 분류기를 만듬

XGboost : Greedy Algorithm이 분류기 M,G,H를 찾고 분산처리를 통해서 적합한 비중 파라미터를 찾는다. 

분류기 M이 Y를 예측한다고 했을 떄
Y = M(x) + error(1)라고 가정. error를 계속 세분화해서 쪼개고 쪼개고 쪼갰다고 했을때 
Y = M(x) + G(x) + H(x) + error3(4) 라고 쓸 수 있고 각 분류기들 앞에 가중치를 만들어두고 XGboost가 최적의 파라미터를 찾는다. 



### XGBoost 코드 예시

###### 신용카드 사기 거래탐지(분류 알고리즘)

~~~python
# xgb
xgb_cfl = xgb.XGBClassifier(n_jobs = -1)

xgb_cfl.fit(X_train, y_train)
y_pred = xgb_cfl.predict(X_test)
y_score = xgb_cfl.predict_proba(X_test)[:,1]
~~~

![image-20180730190949170](/var/folders/b1/wsnpt7md4b3048k2vr4907500000gn/T/abnerworks.Typora/image-20180730190949170.png)



> 참고1 : https://brunch.co.kr/@snobberys/137
>
> 참고2 : Boosting 기법 이해하기 (slideshare)
>
> 참고3 : 개인 프로젝트