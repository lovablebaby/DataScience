{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 구현연습\n",
    "---\n",
    "참고 : [김보섭님 자료(클릭)](\"https://nbviewer.jupyter.org/github/aisolab/CS20/blob/master/Lec11_Recurrent%20Neural%20Networks/To%20quickly%20implementing%20RNN.ipynb\")에서 단순 단어만 조금 바꾸어 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yeoni/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-dev20190102\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "\n",
    "\n",
    "print(list(map(lambda word : len(word), sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro : Padding\n",
    "\n",
    "---\n",
    "`<pad>`라는 의미없는 토큰을 함께 추가해주어서 word_dic을 구성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "## word dic\n",
    "\n",
    "word_list = []\n",
    "for elm in sentences:\n",
    "    word_list += elm\n",
    "\n",
    "word_list = list(set(word_list))\n",
    "word_list.sort()\n",
    "word_list = ['<pad>'] + word_list\n",
    "word_dic = {word : idx for idx, word in enumerate(word_list)}\n",
    "pprint(word_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_len 길이에 못 미치는 문장은 <pad>로 max_len 만큼 padding 해주는 함수를 만든다.\n",
    "# 길이를 맞추는 작업이 다른 딥러닝과 다른 점\n",
    "def pad_seq(sequences, max_len, dic):\n",
    "    seq_len, seq_indices = [], []\n",
    "    for seq in sequences:\n",
    "        seq_len.append(len(seq))\n",
    "        seq_idx = [dic.get(char) for char in seq]\n",
    "        seq_idx += (max_len - len(seq_idx)) * [dic.get('<pad>')]\n",
    "        seq_indices.append(seq_idx)\n",
    "    return seq_len, seq_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5]\n",
      "[[1, 7, 10, 0, 0, 0, 0, 0],\n",
      " [13, 11, 14, 5, 0, 0, 0, 0],\n",
      " [13, 11, 2, 9, 8, 4, 12, 0],\n",
      " [13, 11, 14, 6, 3, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "max_length = 8\n",
    "sen_len, sen_indices = pad_seq(sequences = sentences, max_len = max_length,\n",
    "                               dic = word_dic)\n",
    "pprint(sen_len)\n",
    "pprint(sen_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형식 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = tf.placeholder(dtype = tf.int32, shape=[None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape=[None, max_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "ont_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                         trainable = False) #trainable을 따로 하지 않는다.\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8)\n",
      "(4, 8, 15)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tmp = sess.run(seq_batch, feed_dict = {seq_indices : sen_indices})\n",
    "print(np.shape(sen_indices))\n",
    "print(np.shape(tmp)) # tf.nn.dynamic_rnn, tf.contrib.seq2seq.TrainingHelper 등에 이 shape을 유지하면서 전달되어야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pprint(tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to one\n",
    "\n",
    "---\n",
    "Many to one : example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5]\n",
      "[[1, 7, 10, 0, 0, 0, 0, 0],\n",
      " [13, 11, 14, 5, 0, 0, 0, 0],\n",
      " [13, 11, 2, 9, 8, 4, 12, 0],\n",
      " [13, 11, 14, 6, 3, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "y = [[0.,1.], [0.,1.], [1.,0.], [1.,0.]]\n",
    "max_length = 8\n",
    "\n",
    "sen_len, sen_indices = pad_seq(sequences = sentences, max_len = max_length, dic = word_dic)\n",
    "\n",
    "pprint(sen_len)\n",
    "pprint(sen_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many to one : simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 8\n",
    "h_dim = 2\n",
    "n_of_classes = 2\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-15-dca30818383d>:1: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-15-dca30818383d>:3: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py:558: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py:569: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "<tf.Tensor 'rnn/transpose_1:0' shape=(?, 8, 2) dtype=float32>\n",
      "<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 2) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "_, state = tf.nn.dynamic_rnn(cell = gru_cell, inputs = seq_batch, sequence_length = seq_len,\n",
    "                             dtype = tf.float32)\n",
    "pprint(_)\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = tf.layers.dense(inputs = state, units = n_of_classes)\n",
    "ce_loss = tf.losses.softmax_cross_entropy(onehot_labels=label,\n",
    "                                          logits = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.0662247 , -0.01695941],\n",
      "        [-0.03734433,  0.02484244],\n",
      "        [-0.06660604, -0.01929412],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32),\n",
      " array([[-0.06660604, -0.01929412]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run([_, state], feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679254\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run(ce_loss, feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                      label : y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to one : stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 8\n",
    "n_of_classes = 2\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-ced291dc90ae>:9: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "h_dims = [2,2]\n",
    "gru_cells = []\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell = tf.contrib.rnn.DropoutWrapper(cell = gru_cell,\n",
    "                                             output_keep_prob = keep_prob)\n",
    "    gru_cells.append(gru_cell)\n",
    "else:\n",
    "    gru_cells = tf.contrib.rnn.MultiRNNCell(cells = gru_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1278: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "<tf.Tensor 'rnn/transpose_1:0' shape=(?, 8, 2) dtype=float32>\n",
      "(<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 2) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "_, state = tf.nn.dynamic_rnn(cell = gru_cells, inputs = seq_batch, sequence_length = seq_len,\n",
    "                             dtype = tf.float32)\n",
    "pprint(_)\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = tf.layers.dense(inputs = state[-1], units = n_of_classes)\n",
    "ce_loss = tf.losses.softmax_cross_entropy(onehot_labels = label, logits = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.00917697, -0.00501139],\n",
      "        [ 0.04817685,  0.00979669],\n",
      "        [ 0.02524938, -0.00332088],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32),\n",
      " (array([[ 0.04972173, -0.10506723]], dtype=float32),\n",
      "  array([[ 0.02524938, -0.00332088]], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run([_, state], feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]],\n",
    "                                         keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6831637\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run(ce_loss, feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                      label : y, keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to one : bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 8\n",
    "h_dim = 2\n",
    "n_of_classes = 2\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_fw_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "gru_bw_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "\n",
    "_, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw = gru_fw_cell, cell_bw = gru_bw_cell,\n",
    "                                           inputs = seq_batch, sequence_length = seq_len, dtype = tf.float32)\n",
    "pprint(_)\n",
    "pprint(output_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-cdd4de326e44>:8: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "(<tf.Tensor 'bidirectional_rnn/fw/fw/transpose_1:0' shape=(?, 8, 2) dtype=float32>,\n",
      " <tf.Tensor 'ReverseSequence:0' shape=(?, 8, 2) dtype=float32>)\n",
      "(<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "gru_fw_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "gru_bw_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "\n",
    "_, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw = gru_fw_cell, \n",
    "                                             cell_bw = gru_bw_cell,\n",
    "                                                   inputs = seq_batch,\n",
    "                                                   sequence_length = seq_len,\n",
    "                                                   dtype = tf.float32)\n",
    "pprint(_)\n",
    "pprint(output_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## fw_cell, bw_cell final state를 concat함\n",
    "\n",
    "concat_state = tf.concat(values = [output_states[0],\n",
    "                                  output_states[-1]],\n",
    "                        axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = tf.layers.dense(inputs = concat_state, units = n_of_classes)\n",
    "ce_loss = tf.losses.softmax_cross_entropy(onehot_labels = label,\n",
    "                                         logits = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[[ 0.00608759, -0.12793802],\n",
      "        [ 0.05511357, -0.21184164],\n",
      "        [-0.03150936, -0.1544176 ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32),\n",
      "  array([[[ 0.00733875,  0.01251039],\n",
      "        [-0.05339082,  0.13665897],\n",
      "        [ 0.00767314,  0.05520057],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ]]], dtype=float32)),\n",
      " array([[-0.03150936, -0.1544176 ,  0.00733875,  0.01251039]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run([_, concat_state], feed_dict= {seq_len:[sen_len[0]],\n",
    "                                              seq_indices : [sen_indices[0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7031779\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run(ce_loss, feed_dict=\n",
    "               {seq_len : sen_len, seq_indices:sen_indices,\n",
    "               label : y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to one : stacked bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 8\n",
    "n_of_classes = 2\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "\n",
    "label = tf.placeholder(dtype = tf.float32, shape = [None, 2])\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer=one_hot,\n",
    "                         trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params=one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_dims = [2,2]\n",
    "gru_fw_cells, gru_bw_cells = [], []\n",
    "\n",
    "# forward\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell = tf.contrib.rnn.DropoutWrapper(cell = gru_cell, output_keep_prob = keep_prob)\n",
    "    gru_fw_cells.append(gru_cell)\n",
    "    \n",
    "# backward\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell = tf.contrib.rnn.DropoutWrapper(cell = gru_cell, output_keep_prob = keep_prob)\n",
    "    gru_bw_cells.append(gru_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'stack_bidirectional_rnn/cell_1/concat:0' shape=(?, 8, 4) dtype=float32>\n",
      "(<tf.Tensor 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n",
      "(<tf.Tensor 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "outputs, output_state_fw, output_state_bw = \\\n",
    "tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw = gru_fw_cells, cells_bw = gru_bw_cells,\n",
    "                                               inputs = seq_batch, sequence_length = seq_len,\n",
    "                                               dtype = tf.float32)\n",
    "pprint(outputs)\n",
    "pprint(output_state_fw)\n",
    "pprint(output_state_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_state = tf.concat(values=[output_state_fw[-1],output_state_bw[-1]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = tf.layers.dense(inputs = concat_state, units = n_of_classes)\n",
    "ce_loss = tf.losses.softmax_cross_entropy(onehot_labels = label, logits = score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.01676048, -0.0757765 ,  0.03989091,  0.12585738],\n",
      "        [-0.02815877, -0.06940226,  0.01626515,  0.06314133],\n",
      "        [-0.0399979 , -0.00468424, -0.00685303,  0.00458733],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
      "      dtype=float32),\n",
      " array([[-0.0399979 , -0.00468424,  0.03989091,  0.12585738]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run([outputs, concat_state], feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]],\n",
    "                                                      keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6838166\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run(ce_loss, feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                      label : y, keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to many\n",
    "\n",
    "---\n",
    "\n",
    "#### many to many : example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sentences = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "pos = [['pronoun', 'verb', 'adjective'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective'],\n",
    "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
    "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]\n",
    "max_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_seq(sequences, max_len, dic):\n",
    "    seq_len, seq_indices = [], []\n",
    "    for seq in sequences:\n",
    "        seq_len.append(len(seq))\n",
    "        seq_idx = [dic.get(char) for char in seq]\n",
    "        seq_idx += (max_len - len(seq_idx)) * [dic.get('<pad>')] # 0 is idx of meaningless token \"pad\"\n",
    "        seq_indices.append(seq_idx)\n",
    "    return seq_len, seq_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for': 8, 'fast': 6, 'a': 2, 'is': 11, 'learning': 12, 'framework': 9, 'deep': 4, 'changing': 3, '<pad>': 0, 'tensorflow': 13, 'feel': 7, 'difficult': 5, 'I': 1, 'hungry': 10, 'very': 14}\n",
      "{'preposition': 5, 'verb': 7, 'adverb': 2, 'pronoun': 6, 'adjective': 1, 'determiner': 3, 'noun': 4, '<pad>': 0}\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for elm in sentences:\n",
    "    word_list += elm\n",
    "word_list = list(set(word_list))\n",
    "word_list.sort()\n",
    "word_list = ['<pad>'] + word_list\n",
    "\n",
    "word_dic = {word : idx for idx, word in enumerate(word_list)}\n",
    "\n",
    "# pos dic\n",
    "pos_list = []\n",
    "for elm in pos:\n",
    "    pos_list += elm\n",
    "pos_list = list(set(pos_list))\n",
    "pos_list.sort()\n",
    "pos_list = ['<pad>'] + pos_list\n",
    "\n",
    "pos_dic = {pos : idx for idx, pos in enumerate(pos_list)}\n",
    "\n",
    "print(word_dic)\n",
    "print(pos_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5]\n",
      "[[1, 7, 10, 0, 0, 0, 0, 0],\n",
      " [13, 11, 14, 5, 0, 0, 0, 0],\n",
      " [13, 11, 2, 9, 8, 4, 12, 0],\n",
      " [13, 11, 14, 6, 3, 0, 0, 0]]\n",
      "[[6, 7, 1, 0, 0, 0, 0, 0],\n",
      " [4, 7, 2, 1, 0, 0, 0, 0],\n",
      " [4, 7, 3, 4, 5, 1, 4, 0],\n",
      " [4, 7, 2, 1, 7, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "sen_len, sen_indices = pad_seq(sequences = sentences, max_len = max_length, dic = word_dic)\n",
    "_, pos_indices = pad_seq(sequences = pos, max_len = max_length, dic = pos_dic)\n",
    "\n",
    "pprint(sen_len)\n",
    "pprint(sen_indices)\n",
    "pprint(pos_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to many: simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 8\n",
    "h_dim = 2\n",
    "n_of_classes = len(pos_dic)\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'rnn/transpose_1:0' shape=(?, 8, 8) dtype=float32>\n",
      "<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 2) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell = gru_cell, output_size = n_of_classes)\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell = score_cell, inputs = seq_batch, sequence_length = seq_len,\n",
    "                             dtype = tf.float32)\n",
    "\n",
    "pprint(outputs)\n",
    "pprint(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masking = tf.sequence_mask(lengths = sen_len,\n",
    "                           maxlen = max_length, dtype = tf.float32)\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs,\n",
    "                                                targets = label,\n",
    "                                                weights = masking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.0401592 ,  0.11893751, -0.07371241,  0.06439449,\n",
      "         -0.00943696,  0.0411165 ,  0.08811046, -0.00774744],\n",
      "        [ 0.08153912,  0.07185946, -0.00569717,  0.15431473,\n",
      "          0.05982151,  0.16183802,  0.13667941, -0.10636218],\n",
      "        [ 0.04954111,  0.19189334, -0.12926933,  0.07316232,\n",
      "         -0.03267328,  0.02985734,  0.11993706,  0.01457638],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ]]],\n",
      "      dtype=float32),\n",
      " array([[-0.10720557, -0.16666338]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run([outputs, _], feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 1., 1., 1., 0., 0., 0.]], dtype=float32),\n",
      " 2.0618663]\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run([masking, seq2seq_loss], feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                           label : pos_indices}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to many : stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "max_length = 8\n",
    "n_of_classes = len(pos_dic)\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_dims = [2,2]\n",
    "gru_cells = []\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell =  tf.contrib.rnn.DropoutWrapper(cell = gru_cell, output_keep_prob = keep_prob)\n",
    "    gru_cells.append(gru_cell)\n",
    "else:\n",
    "    gru_cells = tf.contrib.rnn.MultiRNNCell(cells = gru_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'rnn/transpose_1:0' shape=(?, 8, 8) dtype=float32>\n",
      "(<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 2) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell = gru_cells, output_size = n_of_classes)\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell = score_cell, inputs = seq_batch, sequence_length = seq_len,\n",
    "                             dtype = tf.float32)\n",
    "\n",
    "pprint(outputs)\n",
    "pprint(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masking = tf.sequence_mask(lengths = sen_len, maxlen = max_length, dtype = tf.float32)\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = label, weights = masking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.00188777, -0.00037185,  0.00462506, -0.00598995,\n",
      "          0.00910721, -0.00180404,  0.00169455,  0.0033047 ],\n",
      "        [ 0.0004771 ,  0.00058002,  0.00017493, -0.0012932 ,\n",
      "          0.00163719,  0.00043325,  0.00141179,  0.00168271],\n",
      "        [-0.00852313, -0.00381461,  0.01497678, -0.01470956,\n",
      "          0.02381036, -0.0080454 , -0.00043475,  0.00385634],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ]]],\n",
      "      dtype=float32),\n",
      " (array([[ 0.0881837 , -0.05291522]], dtype=float32),\n",
      "  array([[ 0.02257016, -0.0171208 ]], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run([outputs, _], feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]],\n",
    "                                           keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 1., 1., 1., 0., 0., 0.]], dtype=float32),\n",
      " 2.0779552]\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run([masking, seq2seq_loss], feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                                      label : pos_indices, keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to many: bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "max_length = 8\n",
    "h_dim = 2\n",
    "n_of_classes = len(pos_dic)\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'bidirectional_rnn/fw/fw/transpose_1:0' shape=(?, 8, 2) dtype=float32>,\n",
      " <tf.Tensor 'ReverseSequence:0' shape=(?, 8, 2) dtype=float32>)\n",
      "(<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "gru_fw_cell = tf.contrib.rnn.GRUCell(num_units = 2, activation = tf.nn.tanh)\n",
    "gru_bw_cell = tf.contrib.rnn.GRUCell(num_units = 2, activation = tf.nn.tanh)\n",
    "\n",
    "outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw = gru_fw_cell, cell_bw = gru_bw_cell,\n",
    "                                           inputs = seq_batch, sequence_length = seq_len, dtype = tf.float32)\n",
    "pprint(outputs)\n",
    "pprint(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'map/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 8, 8) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "concat_outputs = tf.concat([outputs[0], outputs[1]], axis = 2)\n",
    "weights = tf.get_variable(name = 'weights', shape = (concat_outputs.get_shape()[-1], n_of_classes),\n",
    "                          initializer = tf.contrib.layers.xavier_initializer())\n",
    "score = tf.map_fn(lambda elm : tf.matmul(elm, weights), concat_outputs)\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masking = tf.sequence_mask(lengths = sen_len, maxlen = max_length, dtype = tf.float32)\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = score, targets = label, weights = masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.02485238,  0.08592927, -0.02481479, -0.14082852,\n",
      "          0.05746712, -0.11255324, -0.02309201, -0.0053028 ],\n",
      "        [ 0.09087023,  0.13373238, -0.0172023 , -0.02397757,\n",
      "         -0.02929905, -0.02332759, -0.05857082, -0.00142572],\n",
      "        [ 0.0338428 ,  0.04580762, -0.0417332 ,  0.06195411,\n",
      "         -0.05554413,  0.06113041, -0.05987086,  0.05780768],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ]]],\n",
      "      dtype=float32),\n",
      " (array([[-0.01631796,  0.09252606]], dtype=float32),\n",
      "  array([[ 0.1879795 , -0.06981479]], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run([score, _], feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 1., 1., 1., 0., 0., 0.]], dtype=float32),\n",
      " 2.063561]\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run([masking, seq2seq_loss], feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                                      label : pos_indices}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to many : stacked bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "max_length = 8\n",
    "n_of_classes = len(pos_dic)\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_dims = [2,2]\n",
    "gru_fw_cells, gru_bw_cells = [], []\n",
    "\n",
    "# forward\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell = tf.contrib.rnn.DropoutWrapper(cell = gru_cell, output_keep_prob = keep_prob)\n",
    "    gru_fw_cells.append(gru_cell)\n",
    "    \n",
    "# backward\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell = tf.contrib.rnn.DropoutWrapper(cell = gru_cell, output_keep_prob = keep_prob)\n",
    "    gru_bw_cells.append(gru_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'stack_bidirectional_rnn/cell_1/concat:0' shape=(?, 8, 4) dtype=float32>\n",
      "(<tf.Tensor 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n",
      "(<tf.Tensor 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "outputs, output_state_fw, output_state_bw = \\\n",
    "tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw = gru_fw_cells, cells_bw = gru_bw_cells,\n",
    "                                               inputs = seq_batch, sequence_length = seq_len,\n",
    "                                               dtype = tf.float32)\n",
    "pprint(outputs)\n",
    "pprint(output_state_fw)\n",
    "pprint(output_state_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'map/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 8, 8) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "weights = tf.get_variable(name = 'weights', shape = (outputs.get_shape()[-1], n_of_classes),\n",
    "                          initializer = tf.contrib.layers.xavier_initializer())\n",
    "score = tf.map_fn(lambda elm : tf.matmul(elm, weights), outputs)\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masking = tf.sequence_mask(lengths = sen_len, maxlen = max_length, dtype = tf.float32)\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = score, targets = label, weights = masking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 0.01670285,  0.00166809, -0.02317582, -0.0145303 ,\n",
      "          0.01358973,  0.00069475,  0.00973788, -0.00854048],\n",
      "        [ 0.00082604, -0.00942596, -0.01932247, -0.02213586,\n",
      "          0.02624755,  0.02337823,  0.00904453, -0.02901776],\n",
      "        [-0.00578844, -0.01268585, -0.01192631, -0.02387662,\n",
      "          0.00150439,  0.0192422 ,  0.01630887,  0.00575117],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ]]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run(score, feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]],\n",
    "                                         keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 1., 1., 1., 0., 0., 0.]], dtype=float32),\n",
      " 2.0801873]\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run([masking, seq2seq_loss], feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                                      label : pos_indices, keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### many to many : stacked bi-directional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "max_length = 8\n",
    "n_of_classes = len(pos_dic)\n",
    "\n",
    "seq_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "seq_indices = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "label = tf.placeholder(dtype = tf.int32, shape = [None, max_length])\n",
    "keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "one_hot = np.eye(len(word_dic)).astype(np.float32)\n",
    "one_hot = tf.get_variable(name='one_hot', initializer = one_hot,\n",
    "                                   trainable = False)\n",
    "seq_batch = tf.nn.embedding_lookup(params = one_hot, ids = seq_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_dims = [2,2]\n",
    "gru_fw_cells, gru_bw_cells = [], []\n",
    "\n",
    "# forward\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell = tf.contrib.rnn.DropoutWrapper(cell = gru_cell, output_keep_prob = keep_prob)\n",
    "    gru_fw_cells.append(gru_cell)\n",
    "    \n",
    "# backward\n",
    "for h_dim in h_dims:\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(num_units = h_dim, activation = tf.nn.tanh)\n",
    "    gru_cell = tf.contrib.rnn.DropoutWrapper(cell = gru_cell, output_keep_prob = keep_prob)\n",
    "    gru_bw_cells.append(gru_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'stack_bidirectional_rnn/cell_1/concat:0' shape=(?, 8, 4) dtype=float32>\n",
      "(<tf.Tensor 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n",
      "(<tf.Tensor 'stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>,\n",
      " <tf.Tensor 'stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 2) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "outputs, output_state_fw, output_state_bw = \\\n",
    "tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw = gru_fw_cells, cells_bw = gru_bw_cells,\n",
    "                                               inputs = seq_batch, sequence_length = seq_len,\n",
    "                                               dtype = tf.float32)\n",
    "pprint(outputs)\n",
    "pprint(output_state_fw)\n",
    "pprint(output_state_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor 'map/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, 8, 8) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "weights = tf.get_variable(name = 'weights', shape = (outputs.get_shape()[-1], n_of_classes),\n",
    "                          initializer = tf.contrib.layers.xavier_initializer())\n",
    "score = tf.map_fn(lambda elm : tf.matmul(elm, weights), outputs)\n",
    "pprint(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "masking = tf.sequence_mask(lengths = sen_len, maxlen = max_length, dtype = tf.float32)\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = score, targets = label, weights = masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 0.07602175,  0.03328064, -0.04958954, -0.00882747,\n",
      "          0.0404028 ,  0.03835554,  0.00329314,  0.10084573],\n",
      "        [ 0.04525514,  0.04387965, -0.0269363 , -0.0139736 ,\n",
      "          0.00608762,  0.04037949, -0.03293841,  0.09743132],\n",
      "        [ 0.01821594,  0.05290323, -0.02067207, -0.01135725,\n",
      "         -0.00358083,  0.03827195, -0.05377305,  0.08085747],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
      "          0.        ,  0.        ,  0.        ,  0.        ]]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pprint(sess.run(score, feed_dict = {seq_len : [sen_len[0]], seq_indices : [sen_indices[0]],\n",
    "                                         keep_prob : 1.}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "       [1., 1., 1., 1., 1., 0., 0., 0.]], dtype=float32),\n",
      " 2.0720344]\n"
     ]
    }
   ],
   "source": [
    "pprint(sess.run([masking, seq2seq_loss], feed_dict = {seq_len : sen_len, seq_indices : sen_indices,\n",
    "                                                      label : pos_indices, keep_prob : 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for': 8, 'fast': 6, 'a': 2, 'is': 11, 'learning': 12, 'framework': 9, 'deep': 4, 'changing': 3, '<pad>': 0, 'tensorflow': 13, 'feel': 7, 'difficult': 5, 'I': 1, 'hungry': 10, 'very': 14}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# word dic for sentences\n",
    "source_words = []\n",
    "for elm in sources:\n",
    "    source_words += elm\n",
    "source_words = list(set(source_words))\n",
    "source_words.sort()\n",
    "source_words = ['<pad>'] + source_words\n",
    "\n",
    "source_dic = {word : idx for idx, word in enumerate(source_words)}\n",
    "print(source_dic)\n",
    "print(len(source_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'빠르게': 9, '배가': 7, '<start>': 1, '어렵다': 10, '고프다': 3, '<pad>': 0, '매우': 6, '나는': 4, '프레임워크이다': 13, '위한': 11, '딥러닝을': 5, '<end>': 2, '텐서플로우는': 12, '변화한다': 8}\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# word dic for translations\n",
    "target_words = []\n",
    "for elm in targets:\n",
    "    target_words += elm\n",
    "target_words = list(set(target_words))\n",
    "target_words.sort()\n",
    "target_words =  ['<pad>']+ ['<start>'] + ['<end>'] + \\\n",
    "                    target_words # 번역문의 시작과 끝을 알리는 'start', 'end' token 추가\n",
    "\n",
    "target_dic = {word : idx for idx, word in enumerate(target_words)}\n",
    "print(target_dic)\n",
    "print(len(target_dic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_seq_enc(sequences, max_len, dic):\n",
    "    seq_len = []\n",
    "    seq_indices = []\n",
    "    for seq in sequences:\n",
    "        seq_len.append(len(seq))\n",
    "        seq_idx = [dic.get(word) for word in seq]\n",
    "        seq_idx += (max_len - len(seq_idx)) * [dic.get('<pad>')] \n",
    "        seq_indices.append(seq_idx)        \n",
    "    return seq_len, seq_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_seq_dec(sequences, max_len, dic):\n",
    "    seq_input_len = []\n",
    "    seq_input_indices = []\n",
    "    seq_target_indices = []\n",
    "    \n",
    "    # for decoder input\n",
    "    for seq in sequences:\n",
    "        seq_input_idx = [dic.get('<start>')] + [dic.get(token) for token in seq]\n",
    "        seq_input_len.append(len(seq_input_idx))\n",
    "        seq_input_idx += (max_len - len(seq_input_idx)) * [dic.get('<pad>')] \n",
    "        seq_input_indices.append(seq_input_idx)\n",
    "        \n",
    "    # for decoder output\n",
    "    for seq in sequences:\n",
    "        seq_target_idx = [dic.get(token) for token in seq] + [dic.get('<end>')]\n",
    "        seq_target_idx += (max_len - len(seq_target_idx)) * [dic.get('<pad>')]\n",
    "        seq_target_indices.append(seq_target_idx)\n",
    "        \n",
    "    return seq_input_len, seq_input_indices, seq_target_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] (4, 10)\n"
     ]
    }
   ],
   "source": [
    "# for encoder\n",
    "source_max_len = 10\n",
    "X_length, X_indices = pad_seq_enc(sequences = sources, max_len = source_max_len, dic = source_dic)\n",
    "print(X_length, np.shape(X_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 5, 5]\n",
      "[[1, 4, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 6, 10, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 5, 11, 13, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 12, 6, 9, 8, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[4, 7, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 6, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 5, 11, 13, 2, 0, 0, 0, 0, 0, 0, 0],\n",
      " [12, 6, 9, 8, 2, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# for decoder\n",
    "target_max_len = 12\n",
    "y_length, y_input_indices, y_target_indices = pad_seq_dec(sequences = targets, max_len = target_max_len,\n",
    "                                                             dic = target_dic)\n",
    "pprint(y_length)\n",
    "pprint(y_input_indices)\n",
    "pprint(y_target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "s_indices = tf.placeholder(dtype = tf.int32, shape = [None, source_max_len])\n",
    "t_len = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "t_input_indices = tf.placeholder(dtype = tf.int32, shape = [None, target_max_len])\n",
    "t_output_indices = tf.placeholder(dtype = tf.int32, shape = [None, target_max_len])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_embedding = tf.eye(num_rows = len(source_dic), dtype = tf.float32)\n",
    "s_embedding = tf.get_variable(name = 's_embedding', initializer = s_embedding)\n",
    "s_batch = tf.nn.embedding_lookup(params = s_embedding, ids = s_indices)\n",
    "\n",
    "enc_cell = tf.contrib.rnn.GRUCell(num_units = 2, activation = tf.nn.tanh)\n",
    "_, enc_state = tf.nn.dynamic_rnn(cell = enc_cell, inputs = s_batch, sequence_length = s_len, dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_embedding = tf.eye(num_rows = len(target_dic), dtype = tf.float32)\n",
    "t_embedding = tf.get_variable(name = 't_embedding', initializer = t_embedding)\n",
    "t_batch = tf.nn.embedding_lookup(params = t_embedding, ids = t_input_indices)\n",
    "\n",
    "tokens = tf.ones_like(tensor = s_len, dtype = tf.int32)\n",
    "tr_tokens = tf.map_fn(lambda elm : tf.multiply(elm, target_max_len), tokens, dtype = tf.int32)\n",
    "start_tokens = tokens\n",
    "\n",
    "tr_helper = tf.contrib.seq2seq.TrainingHelper(inputs = t_batch, sequence_length = tr_tokens)\n",
    "dec_cell = tf.contrib.rnn.GRUCell(num_units = 2, activation = tf.nn.tanh)\n",
    "score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell = dec_cell, output_size = len(target_dic))\n",
    "tr_decoder = tf.contrib.seq2seq.BasicDecoder(cell = score_cell, initial_state = enc_state, helper = tr_helper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_outputs,_,_= tf.contrib.seq2seq.dynamic_decode(decoder = tr_decoder, impute_finished = True,\n",
    "                                                  maximum_iterations = target_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masking = tf.sequence_mask(lengths = t_len, maxlen = target_max_len, dtype = tf.float32)\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = tr_outputs.rnn_output,\n",
    "                                                targets = t_output_indices, weights = masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding = t_embedding,\n",
    "                                                        start_tokens = start_tokens,\n",
    "                                                        end_token = target_dic.get('<end>'))\n",
    "trans_decoder = tf.contrib.seq2seq.BasicDecoder(cell = score_cell, initial_state = enc_state,\n",
    "                                                helper = trans_helper)\n",
    "trans_outputs,_,_ = tf.contrib.seq2seq.dynamic_decode(decoder = trans_decoder, impute_finished = True,\n",
    "                                                      maximum_iterations = target_max_len * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "logits, masks = sess.run([tr_outputs.rnn_output,masking], feed_dict = {s_len : [X_length[0]],\n",
    "                                                         s_indices : [X_indices[0]],\n",
    "                                                         t_len : [y_length[0]],\n",
    "                                                         t_input_indices : [y_input_indices[0]]})\n",
    "loss = sess.run(seq2seq_loss, feed_dict = {s_len : [X_length[0]],\n",
    "                                           s_indices : [X_indices[0]],\n",
    "                                           t_len : [y_length[0]],\n",
    "                                           t_input_indices : [y_input_indices[0]],\n",
    "                                           t_output_indices : [y_target_indices[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "translations = sess.run(trans_outputs.sample_id, feed_dict = {s_len : [X_length[0]],\n",
    "                                               s_indices : [X_indices[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 3.06569170e-02,  2.90796943e-02,  2.65586451e-02,\n",
      "          3.55238467e-02,  2.72423159e-02, -2.31380723e-02,\n",
      "          4.59234864e-02, -1.49730723e-02, -1.43889021e-02,\n",
      "          4.43195440e-02,  3.12543707e-03,  2.01934017e-02,\n",
      "         -4.12015757e-03,  1.88582577e-02],\n",
      "        [ 3.35243903e-02, -3.22426520e-02, -2.72103995e-02,\n",
      "          4.89585176e-02,  1.51515007e-04, -5.48993237e-02,\n",
      "          2.30013207e-02, -5.11967838e-02,  4.26761024e-02,\n",
      "          9.92388465e-03, -2.64386237e-02,  5.06320745e-02,\n",
      "          1.63794402e-02, -3.40926908e-02],\n",
      "        [ 3.29880342e-02, -7.31706433e-03, -5.33414260e-03,\n",
      "          4.43210527e-02,  1.14459638e-02, -4.27400693e-02,\n",
      "          3.30073163e-02, -3.71048115e-02,  1.97300129e-02,\n",
      "          2.44550481e-02, -1.46358609e-02,  3.89402136e-02,\n",
      "          8.15707352e-03, -1.26926647e-02],\n",
      "        [-8.30884837e-03,  2.92134397e-02,  2.53850762e-02,\n",
      "         -1.54850120e-02,  9.78414901e-03,  2.34143529e-02,\n",
      "          3.31857242e-03,  2.42285393e-02, -2.99331769e-02,\n",
      "          1.03121065e-02,  1.64464563e-02, -2.20097341e-02,\n",
      "         -1.09803993e-02,  2.65810415e-02],\n",
      "        [ 1.53668514e-02, -3.58101614e-02, -3.09456252e-02,\n",
      "          2.57621873e-02, -9.66365729e-03, -3.48840207e-02,\n",
      "          1.60532910e-03, -3.49030793e-02,  3.87433246e-02,\n",
      "         -8.10759421e-03, -2.19234265e-02,  3.25841345e-02,\n",
      "          1.43663911e-02, -3.35951410e-02],\n",
      "        [ 3.12928185e-02, -7.92055279e-02, -6.85353801e-02,\n",
      "          5.34536652e-02, -2.25863662e-02, -7.39406273e-02,\n",
      "          5.99144027e-04, -7.44920969e-02,  8.46261606e-02,\n",
      "         -2.02908851e-02, -4.75733057e-02,  6.91544488e-02,\n",
      "          3.13042216e-02, -7.37799108e-02],\n",
      "        [ 4.24311087e-02, -1.08811662e-01, -9.41716433e-02,\n",
      "          7.27030784e-02, -3.12800594e-02, -1.00912310e-01,\n",
      "          2.11503357e-04, -1.01775452e-01,  1.16037361e-01,\n",
      "         -2.83640698e-02, -6.51655942e-02,  9.43994299e-02,\n",
      "          4.29076590e-02, -1.01248927e-01],\n",
      "        [ 5.04494756e-02, -1.29299074e-01, -1.11901619e-01,\n",
      "          8.64301920e-02, -3.71563993e-02, -1.19947366e-01,\n",
      "          2.83397734e-04, -1.20967478e-01,  1.37896851e-01,\n",
      "         -3.36789265e-02, -7.74451494e-02,  1.12204954e-01,\n",
      "          5.09915836e-02, -1.20318130e-01],\n",
      "        [ 5.63560948e-02, -1.43612325e-01, -1.24278322e-01,\n",
      "          9.64191481e-02, -4.11248207e-02, -1.33609474e-01,\n",
      "          6.67229295e-04, -1.34681702e-01,  1.53289258e-01,\n",
      "         -3.71255130e-02, -8.61277729e-02,  1.24974087e-01,\n",
      "          5.66926003e-02, -1.33700058e-01],\n",
      "        [ 6.07920587e-02, -1.53678358e-01, -1.32973090e-01,\n",
      "          1.03813104e-01, -4.37888615e-02, -1.43554091e-01,\n",
      "          1.24596059e-03, -1.44609675e-01,  1.64225876e-01,\n",
      "         -3.93026471e-02, -9.23299342e-02,  1.34259209e-01,\n",
      "          6.07512780e-02, -1.43166184e-01],\n",
      "        [ 6.41804934e-02, -1.60790518e-01, -1.39107972e-01,\n",
      "          1.09369934e-01, -4.55568582e-02, -1.50883749e-01,\n",
      "          1.93316303e-03, -1.51879564e-01,  1.72053754e-01,\n",
      "         -4.06185389e-02, -9.67985764e-02,  1.41094580e-01,\n",
      "          6.36634082e-02, -1.49904132e-01],\n",
      "        [ 6.68086857e-02, -1.65832505e-01, -1.43449664e-01,\n",
      "          1.13605104e-01, -4.67085950e-02, -1.56349644e-01,\n",
      "          2.66783312e-03, -1.57260358e-01,  1.77692607e-01,\n",
      "         -4.13536504e-02, -1.00043431e-01,  1.46184817e-01,\n",
      "          6.57674447e-02, -1.54724970e-01]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pprint(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n",
      "2.6351032\n"
     ]
    }
   ],
   "source": [
    "pprint(masks)\n",
    "pprint(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
