{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference : \n",
    "- https://github.com/aisolab/CS20/blob/master/Lec05_Variable%20sharing%20and%20managing%20experiments/How%20to%20use%20keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0-dev20190102\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units = 64, activation = 'relu')) \n",
    "model.add(keras.layers.Dense(units = 64, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(units = 10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "print(tf.get_default_graph().get_operations())\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x125b67320>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "# Create a sigmoid layer:\n",
    "keras.layers.Dense(64, activation='sigmoid')\n",
    "# Or:\n",
    "keras.layers.Dense(64, activation=tf.sigmoid)\n",
    "\n",
    "# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "keras.layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n",
    "\n",
    "# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\n",
    "keras.layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "# A linear layer with a kernel initialized to a random orthogonal matrix:\n",
    "keras.layers.Dense(64, kernel_initializer='orthogonal')\n",
    "\n",
    "# A linear layer with a bias vector initialized to 2.0s:\n",
    "keras.layers.Dense(64, bias_initializer=tf.keras.initializers.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "print(tf.get_default_graph().get_operations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:439: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[<tf.Variable 'dense/kernel:0' shape=(32, 64) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(64, 64) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_2/kernel:0' shape=(64, 10) dtype=float32>, <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32>]\n",
      "[<tf.Operation 'dense_input' type=Placeholder>, <tf.Operation 'dense/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'dense/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'dense/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'dense/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'dense/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'dense/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'dense/kernel/Initializer/random_uniform' type=Add>, <tf.Operation 'dense/kernel' type=VarHandleOp>, <tf.Operation 'dense/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense/kernel/Assign' type=AssignVariableOp>, <tf.Operation 'dense/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/bias/Initializer/zeros' type=Const>, <tf.Operation 'dense/bias' type=VarHandleOp>, <tf.Operation 'dense/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense/bias/Assign' type=AssignVariableOp>, <tf.Operation 'dense/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/MatMul/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/MatMul' type=MatMul>, <tf.Operation 'dense/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/BiasAdd' type=BiasAdd>, <tf.Operation 'dense/Relu' type=Relu>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform' type=Add>, <tf.Operation 'dense_1/kernel' type=VarHandleOp>, <tf.Operation 'dense_1/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_1/kernel/Assign' type=AssignVariableOp>, <tf.Operation 'dense_1/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/bias/Initializer/zeros' type=Const>, <tf.Operation 'dense_1/bias' type=VarHandleOp>, <tf.Operation 'dense_1/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_1/bias/Assign' type=AssignVariableOp>, <tf.Operation 'dense_1/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/MatMul/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/MatMul' type=MatMul>, <tf.Operation 'dense_1/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/BiasAdd' type=BiasAdd>, <tf.Operation 'dense_1/Relu' type=Relu>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform' type=Add>, <tf.Operation 'dense_2/kernel' type=VarHandleOp>, <tf.Operation 'dense_2/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_2/kernel/Assign' type=AssignVariableOp>, <tf.Operation 'dense_2/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/bias/Initializer/zeros' type=Const>, <tf.Operation 'dense_2/bias' type=VarHandleOp>, <tf.Operation 'dense_2/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_2/bias/Assign' type=AssignVariableOp>, <tf.Operation 'dense_2/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/MatMul/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/MatMul' type=MatMul>, <tf.Operation 'dense_2/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/BiasAdd' type=BiasAdd>, <tf.Operation 'dense_2/Softmax' type=Softmax>]\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=64, activation='relu', input_shape = (32,)))\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "print(tf.get_default_graph().get_operations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:125: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure a model for mean-squared error regression.\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
    "              loss='mse',       # mean squared error\n",
    "              metrics=['mae'])  # mean absolute error\n",
    "\n",
    "# Configure a model for categorical classification.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 int32\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Numpy dataset\n",
    "tr_data = np.random.random((1000, 32)).astype(np.float32)\n",
    "tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)\n",
    "\n",
    "val_data = np.random.random((100, 32)).astype(np.float32)\n",
    "val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "\n",
    "tst_data = np.random.random((100, 32)).astype(np.float32)\n",
    "tst_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "\n",
    "print(tr_data.dtype, tr_label.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 169us/sample - loss: 2.3442 - acc: 0.1000 - val_loss: 2.3753 - val_acc: 0.0900\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 57us/sample - loss: 2.3226 - acc: 0.0890 - val_loss: 2.3510 - val_acc: 0.0700\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 45us/sample - loss: 2.3129 - acc: 0.0970 - val_loss: 2.3360 - val_acc: 0.1000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 55us/sample - loss: 2.3073 - acc: 0.0950 - val_loss: 2.3280 - val_acc: 0.1100\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 49us/sample - loss: 2.3040 - acc: 0.1130 - val_loss: 2.3253 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x125f047b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(.01), \n",
    "              loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=tr_data, y=tr_label, epochs=5, batch_size=32, validation_data=(val_data, val_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      " 32/100 [========>.....................] - ETA: 0s - loss: 2.3005 - acc: 0.1250[2.3115787220001223, 0.08]\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and predict\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(x=tst_data, y=tst_label))\n",
    "print(model.predict(x=tst_data).shape)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session() # very important!\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(tf.get_default_graph().get_operations())\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n"
     ]
    }
   ],
   "source": [
    "# tf.data.Dataset instance\n",
    "tr_data = np.random.random((1000, 32)).astype(np.float32)\n",
    "tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_label))\n",
    "tr_dataset = tr_dataset.batch(batch_size=32)\n",
    "tr_dataset = tr_dataset.repeat()\n",
    "\n",
    "val_data = np.random.random((100, 32)).astype(np.float32)\n",
    "val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
    "val_dataset = val_dataset.batch(batch_size=100).repeat()\n",
    "\n",
    "tst_data = np.random.random((100, 32)).astype(np.float32)\n",
    "tst_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "tst_dataset = tf.data.Dataset.from_tensor_slices((tst_data, tst_label))\n",
    "tst_dataset = tst_dataset.batch(batch_size=100)\n",
    "\n",
    "print(tr_dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py:1761: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
      "Epoch 1/5\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3377 - val_loss: 2.3156\n",
      "Epoch 2/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3163 - val_loss: 2.3017\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3080 - val_loss: 2.2948\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3033 - val_loss: 2.2905\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3004 - val_loss: 2.2880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x126304a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(.01), \n",
    "              loss=keras.losses.sparse_categorical_crossentropy)\n",
    "\n",
    "model.fit(tr_dataset, epochs = 5, steps_per_epoch = 1000 // 32,\n",
    "          validation_data = val_dataset, validation_steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss']\n",
      "2.3466379642486572\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and predict\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(tst_dataset, steps = 1))\n",
    "print(model.predict(tst_dataset, steps = 1).shape)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build advanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear\n",
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "Tensor(\"input_1:0\", shape=(?, 32), dtype=float32) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "[<tf.Variable 'dense/kernel:0' shape=(32, 64) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(64, 64) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_2/kernel:0' shape=(64, 10) dtype=float32>, <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32>]\n",
      "[<tf.Operation 'tensors/component_0' type=Const>, <tf.Operation 'tensors/component_1' type=Const>, <tf.Operation 'TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'batch_size' type=Const>, <tf.Operation 'drop_remainder' type=Const>, <tf.Operation 'BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'count' type=Const>, <tf.Operation 'RepeatDataset' type=RepeatDataset>, <tf.Operation 'input_1' type=Placeholder>, <tf.Operation 'dense/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'dense/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'dense/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'dense/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'dense/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'dense/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'dense/kernel/Initializer/random_uniform' type=Add>, <tf.Operation 'dense/kernel' type=VarHandleOp>, <tf.Operation 'dense/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense/kernel/Assign' type=AssignVariableOp>, <tf.Operation 'dense/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/bias/Initializer/zeros' type=Const>, <tf.Operation 'dense/bias' type=VarHandleOp>, <tf.Operation 'dense/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense/bias/Assign' type=AssignVariableOp>, <tf.Operation 'dense/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/MatMul/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/MatMul' type=MatMul>, <tf.Operation 'dense/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense/BiasAdd' type=BiasAdd>, <tf.Operation 'dense/Relu' type=Relu>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'dense_1/kernel/Initializer/random_uniform' type=Add>, <tf.Operation 'dense_1/kernel' type=VarHandleOp>, <tf.Operation 'dense_1/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_1/kernel/Assign' type=AssignVariableOp>, <tf.Operation 'dense_1/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/bias/Initializer/zeros' type=Const>, <tf.Operation 'dense_1/bias' type=VarHandleOp>, <tf.Operation 'dense_1/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_1/bias/Assign' type=AssignVariableOp>, <tf.Operation 'dense_1/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/MatMul/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/MatMul' type=MatMul>, <tf.Operation 'dense_1/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_1/BiasAdd' type=BiasAdd>, <tf.Operation 'dense_1/Relu' type=Relu>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'dense_2/kernel/Initializer/random_uniform' type=Add>, <tf.Operation 'dense_2/kernel' type=VarHandleOp>, <tf.Operation 'dense_2/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_2/kernel/Assign' type=AssignVariableOp>, <tf.Operation 'dense_2/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/bias/Initializer/zeros' type=Const>, <tf.Operation 'dense_2/bias' type=VarHandleOp>, <tf.Operation 'dense_2/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'dense_2/bias/Assign' type=AssignVariableOp>, <tf.Operation 'dense_2/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/MatMul/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/MatMul' type=MatMul>, <tf.Operation 'dense_2/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'dense_2/BiasAdd' type=BiasAdd>, <tf.Operation 'dense_2/Softmax' type=Softmax>]\n",
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/5\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3174\n",
      "Epoch 2/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3166\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 2.3115\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3046\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 2.2956\n"
     ]
    }
   ],
   "source": [
    "data = np.random.random((1000, 32)).astype(np.float32)\n",
    "label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "dataset = dataset.batch(batch_size=32).repeat()\n",
    "print(dataset.output_types)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(32,))  # Returns a placeholder tensor\n",
    "print(inputs, type(inputs))\n",
    "\n",
    "# A layer instance is callable on a tensor, and returns a tensor.\n",
    "x = keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Instantiate the model given inputs and outputs\n",
    "model = keras.Model(inputs = inputs, outputs = predictions)\n",
    "\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "print(tf.get_default_graph().get_operations())\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(.001),\n",
    "              loss=keras.losses.sparse_categorical_crossentropy)\n",
    "# Trains for 5 epochs\n",
    "model.fit(dataset, epochs=5, steps_per_epoch = 1000//32)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Clear\n",
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(tf.get_default_graph().get_operations())\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subclassing tf.keras.Model\n",
    "class MLP(keras.Model):\n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define your layers here.\n",
    "        self.hidden_layer = keras.layers.Dense(units = hidden_dim, activation='relu')\n",
    "        self.output_layer = keras.layers.Dense(units = num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden = self.hidden_layer(inputs)\n",
    "        score = self.output_layer(hidden)\n",
    "        return score\n",
    "    \n",
    "# Instantiate the MLP class\n",
    "mlp = MLP(hidden_dim=100, num_classes=10)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "mlp.compile(optimizer=tf.train.RMSPropOptimizer(.001),\n",
    "            loss=keras.losses.sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n"
     ]
    }
   ],
   "source": [
    "# tf.data.Dataset instance\n",
    "tr_data = np.random.random((1000, 32)).astype(np.float32)\n",
    "tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_label))\n",
    "tr_dataset = tr_dataset.batch(batch_size=32)\n",
    "tr_dataset = tr_dataset.repeat()\n",
    "\n",
    "val_data = np.random.random((100, 32)).astype(np.float32)\n",
    "val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
    "val_dataset = val_dataset.batch(batch_size=100).repeat()\n",
    "\n",
    "tst_data = np.random.random((100, 32)).astype(np.float32)\n",
    "tst_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "tst_dataset = tf.data.Dataset.from_tensor_slices((tst_data, tst_label))\n",
    "tst_dataset = tst_dataset.batch(batch_size=100)\n",
    "\n",
    "print(tr_dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 2.3906 - val_loss: 2.3662\n",
      "Epoch 2/5\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 2.3530 - val_loss: 2.3203\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3249 - val_loss: 2.2991\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3180 - val_loss: 2.3001\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3072 - val_loss: 2.2950\n"
     ]
    }
   ],
   "source": [
    "# Trains for 5 epochs\n",
    "mlp.fit(tr_dataset, epochs=5, steps_per_epoch=1000//32, validation_data = val_dataset, validation_steps=1)\n",
    "\n",
    "del mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### callbacks\n",
    "\n",
    "- A callback is an object passed to a model to customize and extend its behavior during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clear\n",
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(tf.get_default_graph().get_operations())\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tf.data.Dataset instance\n",
    "tr_data = np.random.random((1000, 32)).astype(np.float32)\n",
    "tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_label))\n",
    "tr_dataset = tr_dataset.batch(batch_size=32)\n",
    "tr_dataset = tr_dataset.repeat()\n",
    "\n",
    "val_data = np.random.random((100, 32)).astype(np.float32)\n",
    "val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
    "val_dataset = val_dataset.batch(batch_size=100).repeat()\n",
    "\n",
    "tst_data = np.random.random((100, 32)).astype(np.float32)\n",
    "tst_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "tst_dataset = tf.data.Dataset.from_tensor_slices((tst_data, tst_label))\n",
    "tst_dataset = tst_dataset.batch(batch_size=100)\n",
    "\n",
    "print(tr_dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "31/31 [==============================] - 0s 5ms/step - loss: 2.3445 - val_loss: 2.3362\n",
      "Epoch 2/5\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 2.3245 - val_loss: 2.3225\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3122 - val_loss: 2.3155\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 2.3053 - val_loss: 2.3112\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - 0s 2ms/step - loss: 2.3016 - val_loss: 2.3091\n"
     ]
    }
   ],
   "source": [
    "# Creating \"callback\" object\n",
    "callbacks = [\n",
    "  # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "  keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "# Training\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(.01), \n",
    "              loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              callbacks = callbacks)\n",
    "\n",
    "model.fit(tr_dataset, epochs = 5, steps_per_epoch = 1000 // 32,\n",
    "          validation_data = val_dataset, validation_steps = 1)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save and restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Clear\n",
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(tf.get_default_graph().get_operations())\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subclassing tf.keras.Model\n",
    "class MLP(keras.Model):\n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define your layers here.\n",
    "        self.hidden_layer = keras.layers.Dense(units = hidden_dim, activation='relu')\n",
    "        self.output_layer = keras.layers.Dense(units = num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden = self.hidden_layer(inputs)\n",
    "        score = self.output_layer(hidden)\n",
    "        return score\n",
    "    \n",
    "# Instantiate the MLP class\n",
    "mlp = MLP(hidden_dim=100, num_classes=10)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "mlp.compile(optimizer=tf.train.GradientDescentOptimizer(.001),\n",
    "            loss=keras.losses.sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.float32, tf.int32)\n",
      "(tf.float32, tf.int32)\n"
     ]
    }
   ],
   "source": [
    "# tf.data.Dataset instance\n",
    "tr_data = np.random.random((1000, 32)).astype(np.float32)\n",
    "tr_label = np.random.randint(low=0, high=10, size = 1000).astype(np.int32)\n",
    "tr_dataset = tf.data.Dataset.from_tensor_slices((tr_data, tr_label))\n",
    "tr_dataset = tr_dataset.batch(batch_size=100)\n",
    "tr_dataset = tr_dataset.repeat()\n",
    "\n",
    "val_data = np.random.random((100, 32)).astype(np.float32)\n",
    "val_label = np.random.randint(low=0, high=10, size = 100).astype(np.int32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
    "val_dataset = val_dataset.batch(batch_size=100).repeat()\n",
    "\n",
    "tst_data = np.ones((100,32), dtype=np.float32)\n",
    "tst_label = np.ones((100,), dtype=np.int32)\n",
    "tst_dataset = tf.data.Dataset.from_tensor_slices((tst_data, tst_label))\n",
    "tst_dataset = tst_dataset.batch(batch_size=100)\n",
    "\n",
    "print(tr_dataset.output_types)\n",
    "print(tst_dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 230us/sample - loss: 2.3605 - val_loss: 2.3112\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 24us/sample - loss: 2.3590 - val_loss: 2.3102\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 25us/sample - loss: 2.3576 - val_loss: 2.3092\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 19us/sample - loss: 2.3563 - val_loss: 2.3083\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 14us/sample - loss: 2.3550 - val_loss: 2.3074\n",
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py:1388: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      " 32/100 [========>.....................] - ETA: 0s - loss: 1.46461.464601755142212\n"
     ]
    }
   ],
   "source": [
    "# Trains for 5 epochs\n",
    "mlp.fit(x=tr_data, y=tr_label, epochs=5, batch_size=100,\n",
    "        validation_data=(val_data, val_label))\n",
    "# mlp.fit(tr_dataset, epochs=5, steps_per_epoch=1000//100,\n",
    "#         validation_data=val_dataset, validation_steps=1)\n",
    "mlp.save_weights('../graphs/lecture05/keras/mlp')\n",
    "y_before = np.argmax(mlp.predict(x=tst_data), axis = -1)\n",
    "print(mlp.evaluate(x=tst_data, y=tst_label))\n",
    "# with keras.backend.get_session() as sess:\n",
    "#     before = sess.run(mlp.variables)\n",
    "del mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Yeoni/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "TensorBoard 1.13.0a20190102 at http://Yeoniui-MacBook-Pro.local:6006 (Press CTRL+C to quit)\n",
      "WARNING:tensorflow:From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorboard/plugins/projector/projector_plugin.py:406: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0109 18:49:42.692890 123145393197056 deprecation.py:317] From /Users/Yeoni/anaconda/lib/python3.5/site-packages/tensorboard/plugins/projector/projector_plugin.py:406: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=../graphs/lecture05/keras/mlp --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clear\n",
    "keras.backend.clear_session()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "print(tf.get_default_graph().get_operations())\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Restore\n",
    "## Instantiate the MLP class\n",
    "tst_model = MLP(hidden_dim=100, num_classes=10)\n",
    "tst_model.compile(optimizer=tf.train.GradientDescentOptimizer(.001),\n",
    "                  loss=keras.losses.sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x1265602b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_model.load_weights('../graphs/lecture05/keras/mlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_data = np.ones((100,32), dtype=np.float32)\n",
    "tst_label = np.ones((100,), dtype=np.int32)\n",
    "tst_dataset = tf.data.Dataset.from_tensor_slices((tst_data, tst_label))\n",
    "tst_dataset = tst_dataset.batch(batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.464601755142212\n"
     ]
    }
   ],
   "source": [
    "y_after = np.argmax(tst_model.predict(tst_dataset, steps = 1), axis = -1)\n",
    "print(tst_model.evaluate(tst_dataset, steps = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# equal\n",
    "np.mean(y_before == y_after)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
