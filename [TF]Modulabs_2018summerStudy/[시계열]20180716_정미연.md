
Time Series
===
- 시계열(Time Series) : 일정 시간 간격으로 배치된 데이터들의 수열. 
  - *시간상의 여러 지점을 관측*하거나 *측정*할 수 있는 모든 것
  - 고정 빈도로 표현
  - 일괄적인 간격 혹은 불규칙적인 모습으로도 표현 가능
  - 특정 순간의 타임스탬프, 2018년 전체 등의 고정된 기간, 시간 간격

- 시계열 분석(Time Series Analysis) : 시계열을 해석하고 이해하는 데 쓰이는 여러 가지 방법들을 연구하는 분야
  - "어떤 법칙에서 생성되어서 이런 시계열이 나오느냐? 를 고민
  - 결국엔 시간 종속적

- 시계열 예측(Time Series Prediction) : 주어진 시계열을 보고 수학적인 모델을 만들어 미래에 일어날 것들을 예측하는 것
  - 공학, 과학계산, 주가예측 등의 금융공학
  - 아이스크림의 경우는 여름이 좀더 잘 팔릴거라고 예측하게 되는 경우 등

@ 참고 : 위키피디아, 파이썬 라이브러리를 활용한 데이터 분석

![이미지](/Users/Yeoni/Desktop/image.png)

#### 시계열 데이터의 특성 및 기법
  - 자기상관성 : 이전 결과와 이후 결과에 영향을 미치고 있는 것
    - 확률변수가 있다고 할 때 이전 값이 크면 이후에는 낮은 값이 나온다거나? 평균으로 돌아가려는 모습들을 생각하면 쉽다. 용수철이나 파동그래프 등등
    - 기본 모형(바로 직전 데이터가 그 다음에 영향을 줌) : X(t) = {a 곱 X(t-1)+c}+ u 곱 e(t)
    
  - 이동평균 : 이전에 생긴 불규칙한 사건이 이후의 결과에 편향성을 초래
    - 시간이 지나면서 확률변수의 평균값이 지속적으로 내려가거나 올라가거나 하는 경향들을 의미한다. 보통 전기 사용량은 봄->여름이 갈 수록 높고, 여름->가을은 감소하는 경향을 확인할 수 있다.
    - 기본 모형(바로 직전 데이터가 그 다음에 영향을 줌) : X(t} = {a 곱하기 e(t-1)+c}+u 곱 e(t)
    
  - 불규칙한 패턴을 White Noise라고 말하고, 평균이 0이고 일정한 분산의 정규분포에서 추출된 임의의 수치라고 가정
  
  - 이외에도 ARMA, ARIMA 모형(추세를 반영) 존재

** u : 분산,  e : 에러**

[참고 블로그]("https://m.blog.naver.com/bluefish850/220749045909")

#### 수요 예측기법들 추가 기법

1) 이동평균법 : 제품의 판매량을 기준으로 일정기간별로 산출한 평균 추세를 통해 미래수요를 예측하는 방법으로 과거 판매량자료 중 특정기간의 자료만을 사용하며 이동 평 균의 계산에 활동된 과거자료는 동일한 가중치를 부여한다.

2) 지수평활법 : 주어진 제품의 모든 판매량 자료를 이용하며 기간에 따라 가중치를 두어 평균을 계산하고 추세를 통해 미래수요를 에측하는 방법이다.

3) ARIMA : 판매자료 간의 상관관계를 이용하여 상관요인과 이동평균요인으로 구분하고 이 를 통해 미래수요를 예측하는 방법이다. 여기서 상관요인이란 현재 판매량에 몇 달 전의 판매량이 영향을 미쳤는가를 파악하는 것이고 이동평균요인이란 예측치 와 실제치 간에 어떤 상관관계가 생기는가를 추정하는 것이다.

4) 분해법 : 과거 판매자료가 갖고 있는 변화를 추세변동, 주기변동, 계절변동, 불규칙변 동으로 구분하여 각각을 예측한 후 이를 결합하여 미래수요를 예측하는 방법으로 계절성이 있는 소비재의 경우에 많은 기간의 과거자료가 필요하다.

5) 확산모형 : 제품수명주기이론을 바탕으로 제품이 확산되는 과정을 혁신효과와 모방효과로 구분하여 추정하고 이를 통해 미래수요를 예측하는 방법으로 모형이 변형이 용이하며 시장 환경변화가 많은 경우에 적합한 모형을 쉽게 개발할 수 있다. 주로 신제품의 수요예측에 많이 활용되는데 판매량자료가 없으므로 과거 유사한 제품이 없을 때에는 외국의 사례 등을 통해 유추해낸다.

pandas에서는 시계열을 다룰 수 있는 도구, 알고리즘을 제공합니다.

이 문서는 한빛미디어의 '파이썬 라이브러리를 활용한 데이터 분석' 10장을 참고하여 작성합니다.

#### 날짜, 시간 자료형, 도구

<center>자료형</center> | <center>설명</center>
:---- | :---------
date | 그레고리언 달력을 사용하여 날짜(년, 월, 일)를 지정
time | 하루 중 시간을 시간, 분, 초, 마이크로초 단위로 저장
datetime | 날짜와 시간을 같이 저장
timedelta  | 두 datetime 값 간의 차이(일, 초, 마이크로초)를 표현


```python
import pandas as pd
import numpy as np
```


```python
from datetime import datetime

now = datetime.now()
```


```python
print(now)
print(now.year, now.month, now.day)
```

    2018-07-16 15:41:56.758947
    2018 7 16


#### 문자열을 datetime으로 변환하기


```python
## datetime 사용하기
stamp = datetime(2011,1,3)
print(str(stamp))
print(stamp.strftime('%Y-%m-%d'))
```

    2011-01-03 00:00:00
    2011-01-03



```python
## 날짜를 pandas를 이용하여 타임스탬프 형식으로 바꾸기
datestrs = ['7/6/2018', '8/6/2018']
pd.to_datetime(datestrs)
```




    DatetimeIndex(['2018-07-06', '2018-08-06'], dtype='datetime64[ns]', freq=None)



### 시계열 기초 
- 문자열 혹은 Series 객체로 보통 다루게 된다.


```python
from datetime import datetime

dates = [datetime(2018,1,2), datetime(2018,1,5), datetime(2018,1,7), datetime(2018,1,8),
         datetime(2018,1,10), datetime(2018,1,12)]

ts = pd.Series(np.random.randn(6), index = dates) # randn ; 표준정규분포값 난수생성

ts
```




    2018-01-02    0.964378
    2018-01-05    0.962698
    2018-01-07   -1.349435
    2018-01-08   -2.729322
    2018-01-10   -0.322497
    2018-01-12   -1.005550
    dtype: float64




```python
ts.index
```




    DatetimeIndex(['2018-01-02', '2018-01-05', '2018-01-07', '2018-01-08',
                   '2018-01-10', '2018-01-12'],
                  dtype='datetime64[ns]', freq=None)



### 인덱싱, 선택, 부분선택


```python
## 인덱싱 하기

stamp = ts.index[2]
print(ts[stamp])
print(ts['1/10/2018'])
print(ts['20180110'])
```

    -1.34943457974
    -0.322497285573
    -0.322497285573



```python
### 긴 시계열 선택

longer_ts = pd.Series(np.random.rand(1000), index = pd.date_range('1/1/2018', periods=1000))

print(longer_ts)
print(longer_ts['2018'])
print(longer_ts['2018-05'])
```

    2018-01-01    0.416227
    2018-01-02    0.376499
    2018-01-03    0.136575
    2018-01-04    0.259336
    2018-01-05    0.890590
    2018-01-06    0.832076
    2018-01-07    0.165911
    2018-01-08    0.356238
    2018-01-09    0.860315
    2018-01-10    0.080084
    2018-01-11    0.226140
    2018-01-12    0.901987
    2018-01-13    0.415616
    2018-01-14    0.971219
    2018-01-15    0.924606
    2018-01-16    0.406341
    2018-01-17    0.062333
    2018-01-18    0.405178
    2018-01-19    0.483751
    2018-01-20    0.020352
    2018-01-21    0.504805
    2018-01-22    0.658823
    2018-01-23    0.503055
    2018-01-24    0.132133
    2018-01-25    0.575796
    2018-01-26    0.895643
    2018-01-27    0.030891
    2018-01-28    0.525691
    2018-01-29    0.082851
    2018-01-30    0.499688
                    ...   
    2020-08-28    0.140124
    2020-08-29    0.019165
    2020-08-30    0.583670
    2020-08-31    0.254452
    2020-09-01    0.488504
    2020-09-02    0.442857
    2020-09-03    0.570030
    2020-09-04    0.636753
    2020-09-05    0.552019
    2020-09-06    0.662807
    2020-09-07    0.712165
    2020-09-08    0.278379
    2020-09-09    0.050826
    2020-09-10    0.432020
    2020-09-11    0.065132
    2020-09-12    0.477042
    2020-09-13    0.049579
    2020-09-14    0.422563
    2020-09-15    0.932151
    2020-09-16    0.534952
    2020-09-17    0.970309
    2020-09-18    0.630285
    2020-09-19    0.997556
    2020-09-20    0.349810
    2020-09-21    0.577091
    2020-09-22    0.390089
    2020-09-23    0.908980
    2020-09-24    0.587932
    2020-09-25    0.152013
    2020-09-26    0.506863
    Freq: D, Length: 1000, dtype: float64
    2018-01-01    0.416227
    2018-01-02    0.376499
    2018-01-03    0.136575
    2018-01-04    0.259336
    2018-01-05    0.890590
    2018-01-06    0.832076
    2018-01-07    0.165911
    2018-01-08    0.356238
    2018-01-09    0.860315
    2018-01-10    0.080084
    2018-01-11    0.226140
    2018-01-12    0.901987
    2018-01-13    0.415616
    2018-01-14    0.971219
    2018-01-15    0.924606
    2018-01-16    0.406341
    2018-01-17    0.062333
    2018-01-18    0.405178
    2018-01-19    0.483751
    2018-01-20    0.020352
    2018-01-21    0.504805
    2018-01-22    0.658823
    2018-01-23    0.503055
    2018-01-24    0.132133
    2018-01-25    0.575796
    2018-01-26    0.895643
    2018-01-27    0.030891
    2018-01-28    0.525691
    2018-01-29    0.082851
    2018-01-30    0.499688
                    ...   
    2018-12-02    0.812231
    2018-12-03    0.335296
    2018-12-04    0.773325
    2018-12-05    0.145070
    2018-12-06    0.885490
    2018-12-07    0.340128
    2018-12-08    0.180093
    2018-12-09    0.578752
    2018-12-10    0.818407
    2018-12-11    0.511456
    2018-12-12    0.913002
    2018-12-13    0.905672
    2018-12-14    0.839615
    2018-12-15    0.307537
    2018-12-16    0.225443
    2018-12-17    0.427187
    2018-12-18    0.835643
    2018-12-19    0.411801
    2018-12-20    0.440895
    2018-12-21    0.227616
    2018-12-22    0.415831
    2018-12-23    0.313571
    2018-12-24    0.356023
    2018-12-25    0.150708
    2018-12-26    0.751472
    2018-12-27    0.638986
    2018-12-28    0.820308
    2018-12-29    0.162927
    2018-12-30    0.652786
    2018-12-31    0.515134
    Freq: D, Length: 365, dtype: float64
    2018-05-01    0.261578
    2018-05-02    0.543015
    2018-05-03    0.566416
    2018-05-04    0.918546
    2018-05-05    0.456244
    2018-05-06    0.064909
    2018-05-07    0.263686
    2018-05-08    0.004863
    2018-05-09    0.753442
    2018-05-10    0.431030
    2018-05-11    0.548574
    2018-05-12    0.475590
    2018-05-13    0.562199
    2018-05-14    0.665159
    2018-05-15    0.945701
    2018-05-16    0.803854
    2018-05-17    0.580087
    2018-05-18    0.746349
    2018-05-19    0.688466
    2018-05-20    0.011415
    2018-05-21    0.859033
    2018-05-22    0.613172
    2018-05-23    0.595623
    2018-05-24    0.819092
    2018-05-25    0.306930
    2018-05-26    0.636249
    2018-05-27    0.725950
    2018-05-28    0.059985
    2018-05-29    0.894946
    2018-05-30    0.522294
    2018-05-31    0.614667
    Freq: D, dtype: float64



```python
## 데이터 나누기(부분 선택) - truncate, ix 등을 활용할 수 있음

print(longer_ts.truncate(after='1/7/2018'))
print("=================================")
print(longer_ts.ix['05-2018'])
```

    2018-01-01    0.416227
    2018-01-02    0.376499
    2018-01-03    0.136575
    2018-01-04    0.259336
    2018-01-05    0.890590
    2018-01-06    0.832076
    2018-01-07    0.165911
    Freq: D, dtype: float64
    =================================
    2018-05-01    0.261578
    2018-05-02    0.543015
    2018-05-03    0.566416
    2018-05-04    0.918546
    2018-05-05    0.456244
    2018-05-06    0.064909
    2018-05-07    0.263686
    2018-05-08    0.004863
    2018-05-09    0.753442
    2018-05-10    0.431030
    2018-05-11    0.548574
    2018-05-12    0.475590
    2018-05-13    0.562199
    2018-05-14    0.665159
    2018-05-15    0.945701
    2018-05-16    0.803854
    2018-05-17    0.580087
    2018-05-18    0.746349
    2018-05-19    0.688466
    2018-05-20    0.011415
    2018-05-21    0.859033
    2018-05-22    0.613172
    2018-05-23    0.595623
    2018-05-24    0.819092
    2018-05-25    0.306930
    2018-05-26    0.636249
    2018-05-27    0.725950
    2018-05-28    0.059985
    2018-05-29    0.894946
    2018-05-30    0.522294
    2018-05-31    0.614667
    Freq: D, dtype: float64


### 리샘플링과 빈도 변환

- 리샘플링 : 시계열의 빈도를 변환하는 과정 -> pandas의 resample 메서드

- 다운샘플링 : 상위 빈도의 데이터를 하위 빈도로 집계하는 것
- 업샘플링 : 하위 빈도의 데이터를 상위 빈도로 집계하는 것


```python
### 리샘플링 예제
rng = pd.date_range("1/1/2018", periods = 1000, freq = 'D') # freq D : Day 를 빈도로 한다
ts = pd.Series(np.random.randn(len(rng)), index = rng)
ts.resample('M').mean() # M : MonthEnd, 월 마지막 일
```




    2018-01-31    0.033398
    2018-02-28    0.090580
    2018-03-31   -0.153480
    2018-04-30   -0.314036
    2018-05-31   -0.391173
    2018-06-30    0.008134
    2018-07-31    0.131833
    2018-08-31    0.109650
    2018-09-30   -0.068100
    2018-10-31   -0.266037
    2018-11-30   -0.106849
    2018-12-31    0.165833
    2019-01-31    0.027314
    2019-02-28   -0.053810
    2019-03-31    0.196403
    2019-04-30    0.095033
    2019-05-31    0.262778
    2019-06-30    0.294707
    2019-07-31   -0.048845
    2019-08-31    0.073442
    2019-09-30    0.285094
    2019-10-31    0.301500
    2019-11-30   -0.083047
    2019-12-31    0.349042
    2020-01-31   -0.122408
    2020-02-29   -0.019188
    2020-03-31   -0.118414
    2020-04-30    0.001242
    2020-05-31   -0.058633
    2020-06-30    0.073351
    2020-07-31    0.039942
    2020-08-31    0.390117
    2020-09-30    0.207746
    Freq: M, dtype: float64




```python
### 다운샘플링 예제
# 각 간격의 양 끝 중에서 어디를 열 것인가?
# 집계하려는 구간의 라벨을 간격의 시작으로 할 것인가? 끝으로 할 것인가?

rng = pd.date_range("1/1/2018", periods = 12, freq = 'T') # freq T : 매 분을 빈도로 둔다
ts = pd.Series(np.arange(12), index=rng)
ts
```




    2018-01-01 00:00:00     0
    2018-01-01 00:01:00     1
    2018-01-01 00:02:00     2
    2018-01-01 00:03:00     3
    2018-01-01 00:04:00     4
    2018-01-01 00:05:00     5
    2018-01-01 00:06:00     6
    2018-01-01 00:07:00     7
    2018-01-01 00:08:00     8
    2018-01-01 00:09:00     9
    2018-01-01 00:10:00    10
    2018-01-01 00:11:00    11
    Freq: T, dtype: int64




```python
print(ts.resample('5min').sum())
print(ts.resample('5min', closed='right').sum())
```

    2018-01-01 00:00:00    10
    2018-01-01 00:05:00    35
    2018-01-01 00:10:00    21
    Freq: 5T, dtype: int64
    2017-12-31 23:55:00     0
    2018-01-01 00:00:00    15
    2018-01-01 00:05:00    40
    2018-01-01 00:10:00    11
    Freq: 5T, dtype: int64

